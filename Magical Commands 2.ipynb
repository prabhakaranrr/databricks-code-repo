{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8685aeb-201d-4ea9-89f9-f77d95ff6b0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Welcome to Inceptez Technologies\n",
    "Let us understand about creating notebooks & magical commands\n",
    "    https://fplogoimages.withfloats.com/actual/68009c3a43430aff8a30419d.png,\n",
    "    ![](https://fplogoimages.withfloats.com/actual/68009c3a43430aff8a30419d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45d36c3c-dfa7-475f-a8dd-f7b3297a332c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Let us learn first about Magical Commands\n",
    "    **Important Magic Commands**\n",
    "    - %md: allows you to write markdown text to design the notebook.\n",
    "    - %run: runs a Python file or a notebook.\n",
    "    - %sh: executes shell commands on the cluster nodes.\n",
    "    - %fs: allows you to interact with the Databricks file system (Datalake command)\n",
    "    - %sql: allows you to run Spark SQL/HQL queries.\n",
    "    - %python: switches the notebook context to Python.\n",
    "    - %pip: allows you to install Python packages.\n",
    "    \n",
    "    **Not Important Magic Commands or We learn few of these where we have Cloud(Azure) dependency**\n",
    "    - %scala: switches the notebook context to Scala.\n",
    "    - %r: switches the notebook context to R.\n",
    "    - %lsmagic: lists all the available magic commands.\n",
    "    - %config: allows you to set configuration options for the notebook.\n",
    "    - %load: loads the contents of a file into a cell.\n",
    "    - %who: lists all the variables in the current scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cad6057-680d-4171-adb2-7ad0eabf0bbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to call a notebook from the current notebook using %run magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f83d5b6-6e9e-4ae7-bb7a-f7be48672ee2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/prabhakaranrr@gmail.com/databricks-code-repo/Magic Commands\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df57e170-b96f-46c8-bf88-ce672dc575f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to run a linux commands inside a notebook using %sh magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e862e346-4ecf-4a6e-9ed5-db429f175835",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "ls -l \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8316e049-20bd-47a5-bfba-9a11a9160ca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We are going to use Databricks Unity Catalog (We don't know about it yet)\n",
    "    to create tables and files under the volume (catalog/schema/volume/folder/files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a278b592-91c2-4e0b-90d1-376bf14aa4bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS workspace.default.volumewe47_datalake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fb6be5b-84a7-41d6-baa0-bda62e556da5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Upload some sample data going into (Catalog -> My Organization -> Workspace -> Default -> Volumes) <br> How to run a DBFS (like Hadoop) FS commands inside a notebook using %fs magic command to copy the uploaded data into some other volume from the uploaded volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78824f52-dfec-4342-b9ef-85ef3df94064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls \"dbfs:///Volumes/workspace/default/volumewe47_datalake/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ed3de6d-234a-444e-afc9-fd1ef21d624e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs cp \"dbfs:/Volumes/workspace/default/volumewe47_datalake/patients.csv\" \"dbfs:/Volumes/workspace/default/volumewe47_datalake/patients_copy.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3a7f1aa-6dcc-47e0-9527-ca68da9afdf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Learning for the first time the dbutils, we learn in detail later\n",
    "Rather using fs command, we can use databricks utility command (comprehensive) to copy the data/any other filesystem operations in the DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b48979b0-8cea-4545-871d-d98d9fe0a336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "dbutils.fs.cp(\"dbfs:/Volumes/workspace/default/volumewe47_datalake/patients.csv\", \"dbfs:/Volumes/workspace/default/volumewe47_datalake/patients_copy2.csv\")\n",
    "dbutils.fs.rm(\"dbfs:/Volumes/workspace/default/volumewe47_datalake/patients_copy.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d934f01e-7843-4a68-ba6e-07302ccd599a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to run a Spark SQL/HQL Queries inside a notebook using %sql magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f73f4d-4964-4425-8422-8d4acc2bf4a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create table if not exists default.cities(id int,city string);\n",
    "insert into default.cities values(3,'Mumbai'),(4,'Lucknow');\n",
    "select * from cities;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14be776-37a4-4d44-b2e0-877bb7a3a1d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#OOPS, FBP & Declarative (SQL)\n",
    "#spark.sql(\"select * from cities\").display()\n",
    "spark.sql(\"select * from cities\").select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b360b0e-f94a-47b3-9fc8-a8c69d7f4689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "update cities set city='Kolkata' where id=4;\n",
    "from cities select *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc63ba8e-694f-4af7-bd60-97fd5e6587cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "from cities select *;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "691c6835-2ca5-45bf-8998-e5ae94b3fda8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to run a Python Program inside a notebook using %python  magic command or by default the cell will be enabled with python interpretter only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ce7c742-b62b-45e4-8e33-1e126a2b1977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def sqrt(a):\n",
    "    return a*a\n",
    "print(\"square root function call \",sqrt(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac769e13-1ac5-4c42-8bc6-ded5b9a705a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the python magic cell itself, we already have spark session object instantiated, <br>\n",
    "so we can lavishly write spark programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a97ded9a-dc6b-40f1-b125-f043270aa2d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.session import SparkSession\n",
    "#import pyspark.sql.session as sprk\n",
    "print(spark)\n",
    "spark1 = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    "print(spark1)\n",
    "df1 = spark1.read.csv(\"/Volumes/workspace/default/volumewe47_datalake/patients.csv\",header=True)\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3ed8b97-e481-4c1f-8f32-9e763a147716",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#DSL - Domain specific language\\n\",\n",
    "df1.where(\"married='Yes'\").write.saveAsTable(\"default.we47_patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3395159e-bcb3-4a0f-b2d2-4e5101582e06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select count(1),InPatient from default.we47_patients group by 2\").explain(True)\n",
    "#df1.where(\"married='Yes'\").write.saveAsTable(\"default.we47_patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eff2b60-8de9-4b2f-aa9c-644ef854c0b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.table(\"default.we47_patients\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "632762db-fa85-4aa6-a4f6-fdf29ddb3a37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to install additional libraries in this current Python Interpreter using %pip magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a91203de-b5ab-4e40-8feb-d1b9742653dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pypi"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7062894777534539,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Magical Commands 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
