{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a000b465-9c78-4705-ac5c-4ad26ba9919e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Welcome to Inceptez Technologies\n",
    "Let us understand about creating notebooks & magical commands\n",
    "https://fplogoimages.withfloats.com/actual/68009c3a43430aff8a30419d.png\n",
    "![](https://fplogoimages.withfloats.com/actual/68009c3a43430aff8a30419d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb6579a-931f-4506-acce-dd68c4b76e1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Let us learn first about Magical Commands\n",
    "**Important Magic Commands**\n",
    "- %md: allows you to write markdown text to design the notebook.\n",
    "- %run: runs a Python file or a notebook.\n",
    "- %sh: executes shell commands on the cluster nodes.\n",
    "- %fs: allows you to interact with the Databricks file system (Datalake command)\n",
    "- %sql: allows you to run Spark SQL/HQL queries.\n",
    "- %python: switches the notebook context to Python.\n",
    "- %pip: allows you to install Python packages.\n",
    "\n",
    "**Not Important Magic Commands or We learn few of these where we have Cloud(Azure) dependency**\n",
    "- %scala: switches the notebook context to Scala.\n",
    "- %r: switches the notebook context to R.\n",
    "- %lsmagic: lists all the available magic commands.\n",
    "- %config: allows you to set configuration options for the notebook.\n",
    "- %load: loads the contents of a file into a cell.\n",
    "- %who: lists all the variables in the current scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34022855-58a0-4cb9-aa37-9cf7af58f136",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to call a notebook from the current notebook using %run magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa1fc409-c841-40d5-98d7-1907894fb320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/prabhakaranrr@gmail.com/databricks-code-repo/DATABRICKS_NOTEBOOK_FUNDAMENTALS/2_Explore_Notebook_Markdowns\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8c6e99f-21ff-430e-b5e3-9c615e3d1f47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to run a linux commands inside a notebook using %sh magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b07063a-5150-4090-97ab-32b96d01199d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "ls -l #/databricks-datasets/airlines\n",
    "##head -1 /databricks-datasets/airlines/part-01902\n",
    "#echo \\\"head completed\\\"\n",
    "#tail -1 /databricks-datasets/airlines/part-01902"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8037e32b-85c4-4173-8c61-4aa3a2118875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We are going to use Databricks Unity Catalog (We don't know about it yet)\n",
    "to create tables and files under the volume (catalog/schema/volume/folder/files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30f6aa67-a89c-4600-9468-59f3cc96fc7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS workspace.default.volumeweekend47_datalake  ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d9502e6-09c1-4407-9ba1-dd0e6ce07dd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Upload some sample data going into (Catalog -> My Organization -> Workspace -> Default -> Volumes) <br> How to run a DBFS (like Hadoop) FS commands inside a notebook using %fs magic command to copy the uploaded data into some other volume from the uploaded volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee9afef6-9c54-41f9-a948-2f72667ede02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls \"dbfs:////Volumes/workspace/default/volumeweekend47_datalake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd567204-b13d-41fa-b037-e96d655d0344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs cp \"dbfs:/Volumes/workspace/default/volumeweekend47_datalake/patients.csv\" \"dbfs:/Volumes/workspace/default/volumeweekend47_datalake/patients_copy.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13c3c48c-51e0-4dcb-8499-26229a85c399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Learning for the first time the dbutils, we learn in detail later \n",
    "Rather using fs command, we can use databricks utility command (comprehensive) to copy the data/any other filesystem operations in the DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05acd8bd-7097-453a-9ac0-eb9d00900982",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.cp(\"dbfs:/Volumes/workspace/default/volumeweekend47_datalake/patients.csv\", \"dbfs:/Volumes/workspace/default/volumeweekend47_datalake/patients_copy2.csv\")\n",
    "dbutils.fs.rm(\"dbfs:/Volumes/workspace/default/volumeweekend47_datalake/patients_copy.csv\")\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c9553bb-49c3-498f-a31d-ea33fd1bbade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to run a Spark SQL/HQL Queries inside a notebook using %sql magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5644530e-a7d8-4cc2-8e67-73743502f139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create table if not exists default.city(id int,city string);\n",
    "insert into default.city values(3,'Mumbai'),(4,'Lucknow');\n",
    "select * from city;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2268af32-2ecd-4946-9b87-86680de27642",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#OOPS, FBP & Declarative (SQL)\n",
    "spark.sql(\"select * from city\").explain(True)\n",
    "spark.sql(\"select * from city\").display(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d5848c4-6139-4bcd-8d83-18ca947d5ee3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "update city set city='Kolkata' where id=4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da43bda3-ebba-4a66-bff8-e6f6e6bf9096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "show create table city;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1615d224-0b42-4c7f-a284-6c29dadc0138",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "from city select *;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a330411-15e3-4211-9edf-dc7a6319a038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to run a Python Program inside a notebook using %python  magic command or by default the cell will be enabled with python interpretter only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d8c13df-c9c9-4d07-988e-443d31344ced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def sqrt(a):\n",
    "\treturn a*a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df00698f-3972-4f6a-97ab-26b7e64ba235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"square root function call \",sqrt(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40de782d-6a88-4267-ac8c-89cc9ac77444",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the python magic cell itself, we already have spark session object instantiated, <br>\n",
    "so we can lavishly write spark programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc48c668-dfa0-45c8-93c5-455135ee0de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.session import SparkSession\n",
    "#import pyspark.sql.session as sprk\n",
    "print(spark)\n",
    "spark1 = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    "print(spark1)\n",
    "df1 = spark1.read.csv(\"/Volumes/workspace/default/volumeweekend47_datalake/patients.csv\",header=True)\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e23071a9-9b53-4257-b6f0-cee4df84a8f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#DSL - Domain specific language\n",
    "df1.where(\"married='Yes'\").write.saveAsTable(\"default.weekend47_patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a56a062-0c93-404e-becc-77e7255d0cdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select count(1),InPatient from default.weekend47_patients group by 2\").explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7474cc2-89ca-4607-9975-b340c0dfd9fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.table(\"default.weekend47_patients\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0a5e845-5f26-435d-a2de-5dbaa213c11f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####How to install additional libraries in this current Python Interpreter using %pip magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32415457-006e-4dfb-be21-2efe55a10f6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pypi"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6892047263584399,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1_Explore_Notebooks_magic_commands",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
